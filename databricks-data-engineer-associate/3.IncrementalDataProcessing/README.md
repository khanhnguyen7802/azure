# 3. Incremental Data Processing

## Structured Streaming

### Demo

```python
customers_df = spark.readStream
                      .format("json")
                      .schema(customers_schema)
                      .load("path/to/file")
```

### Write transformed data stream to Delta Table

\_Once the streaming query is executed, we need to stop it manually OR set the params to stop after several iterations.

```python
streaming_query = customers_transformed_df.writeStream
                          .format("delta")
                          .option("checkpointLocation", "<unique_location>")
                          .toTable("<location_to_store>")

streaming_query.stop()
```

### Modify the interval and output

\_To specify the iterations and output mode:

```python
customers_transformed_df.writeStream
                        .format("delta")
                        .outputMode("append")
                        .trigger(processingTime="2 minutes")
                        .option(...)
                        .toTable(...)
```

### Checkpointing

- Stores metadata about streaming query, execution plan
- Tracks processed offsets and committed results

When a batch starts, Spark first reads the `offset log` to get the **start** and **end** offset of the previous batch. It's the `write-ahead log` which is written at the beginning of the batch, so there is no guarantee that the data from the batch was successfully processed.

In order to find that, Spark then reads the `commit log`: if the batch id is in the `offset log` matches with the `commit log`, then that indicates that the previous batch succeeded. Spark starts reading from the end offset of the previous batch.

Otherwise, Spark rereads the data from the start offset of the previous batch. Additionally, Spark also offers once guarantees -> avoid duplication.

### Auto Loader

\_To use AutoLoader:

```python
customers_transformed_df.readStream
                        .format("cloudFiles")
                        .option("cloudFiles.format", "json")
                        .option("cloudFiles.useNotifications", "true") # cloud storage services (e.g., AWS, S3, ...)
                        .schema(...)
                        .load(...)
```

## Delta Lake

### Delta Transaction Log

![alt text](./img/image.png)

Let's say, first we created a `Delta Table` in `Unity Catalog`, and it also creates a folder in Cloud Storage to store the **file** and the **transaction log**.

Then, **Transaction 1 - Insert Data**, **Transaction 2 - Insert more data**, these transaction logs give Delta Lake the ability to offer ACID transactions as well as time travel capabilities.

![alt text](./img/image-1.png)

### Delta Lake Version History

\_To query the table history:

```sql
DESCRIBE HISTORY <table_name>
```

\_To query data from a specific version:

```sql
SELECT *
FROM ...
VERSION AS OF 1;

-- or use the timestamp
SELECT *
FROM ...
TIMESTAMP AS OF '<the_specific_timestamp>';

```

\_To restore the version:

```sql
RESTORE TABLE <table_name>
VERSION AS OF 1;

```
### Delta Lake ACID Transactions
- Firstly, the **Transaction Logs** only return at the end of the transaction. E.g., when you insert data into the table, data files will be return first and only AFTER successful completion of writes in the data files, the **Transaction logs** will be written -> transaction logs become the single source of truth.
- When reading the data files, Delta Lake will read the **Transaction logs**, therefore it will not read the corrupted files (since the **transaction logs** won't be created).

### Create - Drop table 
- `CREATE OR REPLACE` retains all the transaction history, whereas `DROP` and then `CREATE` the table removes all of that history. -> should use create/replace to retain the history. 

### Delta Lake Table & Column properties
_To see the comment of a table:
```sql
DESC EXTENDED <table_name>
```

_`TBL properties` (table properties): used to specify table level metadata or configuration settings.
```sql
CREATE TABLE <table_name>
(...)
TBLPROPERTIES ('sensitive' = 'true'); -- can also change table configs
```

_To create the **column properties**:
```sql
CREATE TABLE <table_name>
(company_id BIGINT NOT NULL GENERATED ALWAYS AS IDENTITY (START WITH 1 INCREMENT BY 1),
...)

```
When using the clause `GENERATED BY DEFAULT AS IDENTITY`, insert operations can specify values for the identity column. Modify the clause to be `GENERATED ALWAYS AS IDENTITY` to override the ability to manually set values.

_**CTAS (Create Table As statement)**: a single operation 
```sql
CREATE TABLE <table_name> AS
(
  SELECT *
  FROM ...
  WHERE ...
)
```
Drawbacks: you **CANNOT** specify the columns properties; if you want to define the column data types, then it must be done in the **SELECT** statement. OR, you need to alter the table after creation. 

```sql
ALTER TABLE <table_name>
ALTER COLUMN <col_name> TYPE BIGINT
```

### Insert Overwrite and Partitioning 
- **Insert Overwrite**: overwrites the **existing** data in a table or a specific partition with the new data. 
- **Insert Into**: appends new data 

_If we want to insert overwrite the partitions in different tables:
```sql
INSERT OVERWRITE TABLE <table_partition_name>
PARTITION (country="USA")
SELECT ...
FROM <table_from_usa>
```

> Handle schema changes 
<br> - **Insert Overwrite**: Use to overwrite the data in a table or a partition when there are no schema changes. 
<br> - **Create or replace** table: use when there are schema changes.

### Copy into and Merge 
_Copy into (incrementally load new files from Cloud Storage into Delta table):
```sql
COPY INTO <table_name>
FROM 'path/to/file'
FILEFORMAT = JSON
FORMAT_OPTIONS ('inferSchema' = 'true')
COPY_OPTIONS ('mergeSchema' = 'true') -- if a new column is found, just add into the table
```
<br>
_Merge into: merge the source data into target table

```sql
MERGE INTO <target_table> AS target
USING <source_data> AS source
ON <tgt.id = src.id>
WHEN MATCHED AND source.status ='ACTIVE' THEN
  UPDATE SET 
    target.price = source.price,
    target.trading_date = source.trading_date

WHEN MATCHED AND source.status = 'DELISTED' THEN 
  DELETE 

WHEN NOT MATCHED THEN
  INSERT (...) VALUES (...);

```

### Delta Lake Compaction
_File compaction using `OPTIMIZE`: merge multiple small files into fewer, larger files. The history is still maintained. 
```sql
OPTIMIZE <table_name>
```

_File compaction using `ZORDER BY`: the data with specific z-order column values are kept together in the same file. -> optimize by searching through fewer data files.
```sql
OPTIMIZE <table_name>
ZORDER BY <column_name>
```

### Vacuum command 
- Used to removed, unused files from Delta Lake to free up storage. <br> E.g., when executing `OPTIMIZE`, the files are compacted into a larger file, leaving redundant smaller files.
- This command permanently removes data from the cloud storage, and it removes the files which are not referenced in the latest transaction log and older than the retention threshold (by default 7 days).

```sql
SET spark.databricks.delta.retentionDurationCheck.enabled = false; -- (optionally) disable the warning
VACUUM <table_name> RETAIN 1 HOURS; -- leave history for the last hour and delete files before that
```


## Delta Live Table
What is Delta Live Table?
- A **declarative** ETL framework for building reliable, maintainable, and testable data processing pipelines. 
- You define the transformation to perform on your data and Delta Live Tables manage task orchestration, cluster management, monitoring, data quality and error handling. 

![alt text](./img/image-2.png)

### Programming with DLT 
![alt text](./img/image-3.png)

```sql
CREATE OR REFRESH Live Dataset
[Data Quality Expectation]
AS 
  SELECT columns, transformations 
  FROM source  
```

3 types of Live Dataset: 
- `Streaming Tables`: Delta Table to which streams write data. Reads from EventHubs, Kafka, Cloud Files, ... Suitable for Incremental Data Ingestion; Allow DML operations; Massive amount of data  
- `Materialized Views`: Delta Table created from the result of a query. Suitable for full refresh data ingestion workloads. NO DML operations allowed  
- `Standard Views`: no physical storage of the data; scope limited to pipeline; cannot be published to Hive Metastore/Unity Catalog. Suitable for intermediate results to reduce complexity and enforce data quality constraints.  

### DLT Pipelines and Notebooks 
>Within a DLT notebook, it is NOT ALLOWED to mix different languages (e.g., you cannot have a cell in SQL then a cell in Python). So, **NO magic commands**.

_Create **streaming table** (note that we *cannot provide a schema* here as schema is configured in DLT pipeline when we create it). We can only provide 1 schema in the pipeline for all the notebooks that are attached to that pipeline.
```sql
CREATE OR REFRESH STREAMING TABLE <table_name>
COMMENT ''
TBLPROPERTIES('quality'='bronze')
AS 
  SELECT ...
  FROM cloud_files(
    'path/to/file/data',
    'json',
    map('cloudFiles.inferColumnTypes', 'true')
  );
```
In case when you're at the silve table and want to choose from bronze table, use LIVE (in the same pipeline), and `STREAM` modifier helps with only reading the latest data since the last execution (as a stream rather reading whole data, to load data incrementally):
```sql 
...
  SELECT 
  FROM STREAM(LIVE.bronze_table)
```

Implementation using **Python** syntax: 
```python
import dlt

@dlt.table(
    name = 'new_bronze_address',
    comment = '',
    table_properties = {'quality': 'bronze'}
) # create a DLT dataset
def bronze_address():
  return(
      spark.readStream # indicates streaming table, otherwise spark.read()
              .format("cloudFiles")
              .option("cloudFiles.format", "csv")
              .option("cloudFiles.inferColumnTypes", "true")
              .load("/path/to/files/being/read")
  ) # return a dataframe
```
The name of the table will be the same as the function's name (i.e., `bronze_address`). In case you want different config, you can pass in additional args.



**Reading data from a streaming Delta Table**:
```python
@dlt.table()

def silver_address_clean():
  return (
    spark.readStream.table("LIVE.bronze_address") 
  )
```

We can only execute that code block using `DLT pipeline`. Click `Workflow` on the left tab -> `Pipelines`-> `Create piplines` -> `ETL Pipeline`.

For **Compute option**. in **Cluster mode**:
- Enhanced autoscaling: optimize the cluster utilization by automatically allocating cluster resources based on your workload volume.
- Fixed size: the size of the cluster is fixed

In **Advanced option**, we config `pipelines.clusterShutdown.delay` to be 30m. 

Then, after having done with the configs, we start the pipeline:
- Default refresh mode: process the new data from an incremental source
- Full refresh mode: reset all the checkpoint, delete all the data from the streaming tables and reprocess all the available data.


_To add a new Notebook into the pipeline, go to `Setting` on top right, then `Add source code` -> choose the notebook.

### DLT Expectations - Validating
`Expectations` are applied for each of the record being processed. Expectation is basically just a conditional expression (e.g., id not null).

```sql
CONSTRAINT <expectation_name>
  EXPECT (<conditional_expression>)
  [ON VIOLATION (FAIL UPDATE | DROP ROW)]

CONSTRAINT valid_customer_id
EXPECT (customer_id IS NOT NULL)
ON VIOLATION FAIL UPDATE
```

To add the data quality rules:
```sql
CREATE OR REFRESH STREAMING TABLE silver_customer_clean(
  CONSTRAINT valid_customer_id EXPECT (customer_id IS NOT NULL) 
    ON VIOLATION FAIL UPDATE -- fail the data flow 
  CONSTRAINT valid_customer_name EXPECT (customer_name IS NOT NULL) 
    ON VIOLATION DROP ROW  
  CONSTRAINT valid_telephone EXPECT (LENGTH(telephone) >= 10) -- get a warning in case violation  
)
```

Using Python:
```python
@dlt.expect("valid_customer_id", "customer_id IS NOT NULL")
@dlt.expect_or_drop("valid_customer_name", "customer_name IS NOT NULL")
@dlt.expect_or_fail("valid_telephone", "LENGTH(telephone) >= 10")

valid_records = {
                  "valid_customer_id": "customer_id IS NOT NULL",
                  "valid_customer_name": "customer_name IS NOT NULL",
                  "valid_telephone": "LENGTH(telephone) >= 10"
                }
@dlt.expect_all_or_drop(valid_records) # if any of them fail, the record will be dropped; also applies with similar decorators
```

### Type 1 SCD 
It overwrites the data, which means that no history will be maintained.

**Apply changes API** - check in docs for extensive options.
```sql
APPLY CHANGES INTO <target_table>
FROM <src_table>
KEYS <columns>
SEQUENCE BY <columns>
STORED AS <SCD TYPE 1 | SCD TYPE 2>

APPLY CHANGES INTO LIVE.silver_customers
FROM STREAM(LIVE.silver_customers_clean)
KEYS(customer_id)
SEQUENCE BY created_date
STORED AS SCD TYPE 1;
```

### Type 2 SCD
The records will have **start_date** and **end_date**, where the new updated record will be added with a new **start_date**.
<br> -> We will have the full history of that record based on time.  

**Using Python syntax with `create_streaming_table()` and `apply_changes()`**:
```python
dlt.create_streaming_table(
    name = "silver_address",
    comment = "SCD type 2 address table",
    table_properties = {"quality": "silver"}
)

dlt.apply_changes(
  target = "silver_address",
  source = "silver_address_clean",
  keys = ["customer_id"],
  sequence_by = "created_date",
  stored_as_scd_type = 2
)
```


### Type 3 SCD
Only the most recent change is tracked.

## Lecgacy terminology
|     |           New terminology           |      Legacy terminology       |
| :-- | :--------------------------------:  | :---------------------------------: |
|     |          Materialized View          |           LIVE table           |
|     | CREATE OR REFRESH MATERIALIZED VIEW |  CREATE OR REFRESH LIVE TABLE   |
|     | Streaming table |  Streaming LIVE table   |
|     | CREATE OR REFRESH STREAMING TABLE   | CREATE OR REFRESH STREAMING LIVE TABLE | 
|


