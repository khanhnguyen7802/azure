{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6de97cb8-cd1d-4433-903e-68f177bd2231",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Notebook containing all (defined) needed functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53ae40c5-edc9-425a-a11e-768a39f4a9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "\n",
    "def mounting_all_containers(storage_account,  key): \n",
    "    # Connect to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient(\n",
    "        f\"https://{storage_account}.blob.core.windows.net\", credential=key\n",
    "    )\n",
    "\n",
    "    # Get all containers\n",
    "    containers = [container.name for container in blob_service_client.list_containers()]\n",
    "\n",
    "    # Mount ADLS only if the container is not already mounted\n",
    "    for container in containers:\n",
    "        source = f\"wasbs://{container}@{storage_account}.blob.core.windows.net/\"\n",
    "        mount_folder = f\"/mnt/{storage_account}/{container}\"  \n",
    "        config = {\"fs.azure.account.key.\" + storage_account + \".blob.core.windows.net\" : key}\n",
    "\n",
    "        if not any(mount.mountPoint == mount_folder for mount in dbutils.fs.mounts()): \n",
    "            try:\n",
    "                dbutils.fs.mount(\n",
    "                    source = source,\n",
    "                    mount_point = mount_folder,\n",
    "                    extra_configs = config)\n",
    "                print(f\"Mount to {mount_folder} succeeded!\")\n",
    "            except Exception as e:    \n",
    "                print(f\"Mount to {mount_folder} failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d85c8232-5fa0-405c-8cb3-aaa55af9b89d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_trunc, from_utc_timestamp, current_timestamp\n",
    "\n",
    "def add_ingestion_date(input_df):\n",
    "    output_df = input_df.withColumn(\"ingestion_date\", date_trunc(\"second\", \n",
    "                                        from_utc_timestamp(current_timestamp(), \"Europe/Amsterdam\")\n",
    "                                        ))\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd5bdede-ce1b-44b1-a255-5d9b191523e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rearrange_partition_column(input_df, partition_column): # move the partition column to the end of the column list\n",
    "    column_list = []\n",
    "    for column_name in input_df.schema.names:\n",
    "        if column_name != partition_column:\n",
    "            column_list.append(column_name)\n",
    "\n",
    "    column_list.append(partition_column)\n",
    "\n",
    "    output_df = input_df.select(column_list)\n",
    "\n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a67eefd-c7ca-4200-9733-71c19be62647",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def overwrite_partition(input_df, db_name, table_name, partition_column): # apply inplace\n",
    "    output_df = rearrange_partition_column(input_df, partition_column)\n",
    "\n",
    "    # set to dynamic partitioning\n",
    "    spark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") # when insertInto runs, it's going to find the partitions and only replace those partitions with the new data received (i.e., not overwrite the entire table)\n",
    "\n",
    "    if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "        # append any subsequent data\n",
    "        output_df.write.mode(\"overwrite\").insertInto(f\"{db_name}.{table_name}\") # based on the last column to be the partitioned column according to spark\n",
    "    else:\n",
    "        # incremental load for the first time\n",
    "        output_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb7505a4-145d-498d-bbd8-17bf58a309d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def df_column_to_list(input_df, column_name):\n",
    "    df_row_list = input_df.select(column_name).distinct().collect()\n",
    "\n",
    "    column_value_list = [row[column_name] for row in df_row_list]\n",
    "\n",
    "    return column_value_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5a0af2f-aed2-456f-a87d-72e0142ab9f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def merge_delta_data(input_df, db_name, table_name, folder_path, merge_condition, partition_column):\n",
    "    # set to dynamic partitioning\n",
    "    spark.conf.set(\"spark.databricks.optimizer.dynamicPartitionPruning\", \"true\") \n",
    "\n",
    "    from delta.tables import DeltaTable \n",
    "\n",
    "    if (spark._jsparkSession.catalog().tableExists(f\"{db_name}.{table_name}\")):\n",
    "        # append any subsequent data\n",
    "        deltaTable = DeltaTable.forPath(spark, f\"{folder_path}/{table_name}\")\n",
    "        deltaTable.alias(\"tgt\").merge(\n",
    "            input_df.alias(\"src\"),\n",
    "            merge_condition) \\\n",
    "                .whenMatchedUpdateAll() \\\n",
    "                .whenNotMatchedInsertAll() \\\n",
    "                .execute()\n",
    "    else:\n",
    "        # incremental load for the first time\n",
    "        input_df.write.mode(\"overwrite\").partitionBy(partition_column).format(\"delta\").saveAsTable(f\"{db_name}.{table_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b9e7d00f-af33-4c04-833e-a6a449b5eebf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8866502381636162,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
